"""
Drift-source end-to-end runner (clang-based).

Each case lives under `lang2/codegen/tests/e2e/<case>/` and must provide:
  - main.drift   (parsed with the copied lang2 parser)
  - expected.json with exit_code/stdout/stderr fields

The runner:
  1) Parses main.drift with the lang2 parser copy.
  2) Lowers to HIR via AstToHIR.
  3) Runs the full stubbed pipeline to LLVM IR (`compile_to_llvm_ir_for_tests`).
  4) Compiles IR with clang and executes the binary.
  5) Asserts diagnostics are empty and compares exit/stdout/stderr to expected.json.

	Artifacts are written to `build/tests/lang2/codegen/e2e/<case>/`.
	"""

from __future__ import annotations

import argparse
import json
import os
import shutil
import subprocess
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import Iterable, Optional

from lang2.driftc.parser import parse_drift_to_hir, parse_drift_files_to_hir, parse_drift_workspace_to_hir
from lang2.driftc.module_lowered import flatten_modules
from lang2.driftc.driftc import compile_to_llvm_ir_for_tests, ReservedNamespacePolicy
from lang2.driftc.core.function_id import function_symbol
from lang2.drift_core.runtime import get_runtime_sources


ROOT = Path(__file__).resolve().parents[4]
BUILD_ROOT = ROOT / "build" / "tests" / "lang2" / "codegen" / "e2e"


def _run_ir_with_clang(ir: str, build_dir: Path, argv: list[str] | None = None) -> tuple[int, str, str]:
	"""Compile the provided LLVM IR with clang and return (exit, stdout, stderr)."""
	clang = shutil.which("clang-15") or shutil.which("clang")
	if clang is None:
		return 1, "", "clang not available"

	build_dir.mkdir(parents=True, exist_ok=True)
	ir_path = build_dir / "ir.ll"
	bin_path = build_dir / "a.out"
	ir_path.write_text(ir)

	runtime_sources = get_runtime_sources(ROOT)
	# The runtime sources include vendored C code (e.g. Ryu) that expects the
	# directory containing the `ryu/` folder to be on the include path.
	runtime_include = ROOT / "lang2" / "drift_core" / "runtime"
	compile_res = subprocess.run(
		[
			clang,
			"-I",
			str(runtime_include),
			"-x",
			"ir",
			str(ir_path),
			"-x",
			"c",
			*(str(p) for p in runtime_sources),
			"-o",
			str(bin_path),
		],
		capture_output=True,
		text=True,
		cwd=ROOT,
	)
	if compile_res.returncode != 0:
		return compile_res.returncode, "", compile_res.stderr

	run_res = subprocess.run(
		[str(bin_path), *(argv or [])],
		capture_output=True,
		text=True,
		cwd=ROOT,
	)
	return run_res.returncode, run_res.stdout, run_res.stderr


def _run_case(case_dir: Path) -> str:
	expected_path = case_dir / "expected.json"
	source_path = case_dir / "main.drift"
	drift_files = sorted(case_dir.rglob("*.drift"))
	if not expected_path.exists() or not source_path.exists():
		return "skipped (missing expected.json or main.drift)"
	if not drift_files:
		return "skipped (missing .drift sources)"

	expected = json.loads(expected_path.read_text())
	if expected.get("skip"):
		return "skipped (marked)"
	allow_reserved = bool(expected.get("dev_allow_reserved_namespaces", False))
	module_paths = expected.get("module_paths") or []
	module_args: list[str] = []
	for mp in module_paths:
		module_args.extend(["-M", str(case_dir / mp)])
	# Compile-error path: delegate to driftc --json for structured diags.
	if expected.get("diagnostics") and expected.get("use_driftc_json", True):
		def _match_diag_span(exp: dict, d: dict) -> bool:
			# Optional span assertions for diagnostics emitted via driftc --json.
			#
			# These are best-effort and intentionally minimal: we only compare file/line/column
			# when the expectation includes them. This keeps tests stable while still locking
			# the key “pinned span” invariants for important parse/typecheck errors.
			if "file" in exp and exp["file"] != d.get("file"):
				return False
			if "line" in exp and exp["line"] != d.get("line"):
				return False
			if "column" in exp and exp["column"] != d.get("column"):
				return False
			return True

		cmd = [
			str(Path(sys.executable)),
			"-m",
			"lang2.driftc.driftc",
			*module_args,
			*[str(p) for p in drift_files],
			"--json",
		]
		if allow_reserved:
			cmd.insert(3, "--dev")
		res = subprocess.run(cmd, cwd=ROOT, capture_output=True, text=True)
		try:
			payload = json.loads(res.stdout)
		except json.JSONDecodeError as err:
			return f"FAIL (expected JSON diagnostics, got parse error: {err})"
		exit_expected = expected.get("exit_code", 1)
		if payload.get("exit_code") != exit_expected:
			return f"FAIL (exit {payload.get('exit_code')} != expected {exit_expected})"
		diag_expect = expected.get("diagnostics", [])
		diags = payload.get("diagnostics", [])
		for exp in diag_expect:
			msg_sub = exp.get("message_contains")
			phase = exp.get("phase")
			match_found = False
			for d in diags:
				if phase is not None and d.get("phase") != phase:
					continue
				if msg_sub is not None and msg_sub not in d.get("message", ""):
					continue
				if not _match_diag_span(exp, d):
					continue
				match_found = True
				break
			if not match_found:
				return "FAIL (missing expected diagnostic)"
		return "ok"
	# Always parse using the workspace loader (even for single-file cases) so
	# import resolution behavior is consistent:
	# - missing module imports are diagnosed,
	# - multi-file modules and multi-module cases share the same entry path.
	allow_reserved = bool(expected.get("dev_allow_reserved_namespaces", False))
	modules, type_table, exception_catalog, module_exports, module_deps, parse_diags = parse_drift_workspace_to_hir(
		drift_files,
		module_paths=[case_dir / mp for mp in module_paths] or None,
	)
	func_hirs, signatures, fn_ids_by_name = flatten_modules(modules)
	origin_by_fn_id: dict[object, Path] = {}
	for mod in modules.values():
		origin_by_fn_id.update(mod.origin_by_fn_id)
	expected_phase = expected.get("phase")

	if parse_diags:
		actual_exit = 1  # Parsing produced diagnostics; treat as compile failure.
		expected_exit = expected.get("exit_code", actual_exit)
		actual_stdout = ""
		actual_stderr = "\n".join(d.message for d in parse_diags)
		if expected_phase not in (None, "parser"):
			return f"FAIL (expected phase {expected_phase}, but diagnostics occurred during parser phase)"
		if expected_exit != actual_exit:
			return f"FAIL (parser phase exit {actual_exit}, expected {expected_exit})"
		if expected.get("stdout", actual_stdout) != actual_stdout:
			return "FAIL (parser phase stdout mismatch)"
		if expected.get("stderr", actual_stderr) != actual_stderr:
			return "FAIL (parser phase stderr mismatch)"
		return "ok"

	main_ids = [fn_id for fn_id, sig in signatures.items() if fn_id.name == "main" and not sig.is_method]
	main_id = main_ids[0] if len(main_ids) == 1 else None
	entry = function_symbol(main_id) if main_id is not None else "main"
	main_sig = signatures.get(main_id) if main_id is not None else None
	needs_argv = False
	if main_sig and main_sig.param_type_ids and len(main_sig.param_type_ids) == 1 and type_table is not None:
		param_ty = main_sig.param_type_ids[0]
		td = type_table.get(param_ty)
		if td.kind.name == "ARRAY" and td.param_types:
			elem_def = type_table.get(td.param_types[0])
			if elem_def.name == "String":
				needs_argv = True
	try:
		reserved_policy = (
			ReservedNamespacePolicy.ALLOW_DEV
			if allow_reserved
			else ReservedNamespacePolicy.ENFORCE
		)
		ir, checked = compile_to_llvm_ir_for_tests(
			func_hirs=func_hirs,
			signatures=signatures,
			exc_env=exception_catalog,
			entry=entry,
			type_table=type_table,
			module_exports=module_exports,
			module_deps=module_deps,
			origin_by_fn_id=origin_by_fn_id,
			enforce_entrypoint=True,
			reserved_namespace_policy=reserved_policy,
		)
	except Exception as err:  # pragma: no cover - defensive for negative e2e cases
		expected_diags = expected.get("diagnostics", [])
		if expected_diags:
			msg = str(err)
			for exp in expected_diags:
				if exp.get("message_contains") and exp["message_contains"] not in msg:
					return f"FAIL (missing expected diagnostic; saw exception {msg})"
			return "ok"
		raise

	# If the checker produced diagnostics and the test expects them, validate and
	# short-circuit before running codegen output.
	checked_diags = getattr(checked, "diagnostics", [])
	expected_diags = expected.get("diagnostics")
	if expected_diags is not None:
		expected_phase = expected.get("phase", "typecheck")
		expected_exit = expected.get("exit_code", 1)
		diag_has_error = any(getattr(d, "severity", None) == "error" for d in checked_diags)
		if expected_exit != (1 if diag_has_error else 0):
			return f"FAIL (expected exit {expected_exit} but diagnostics imply exit {1 if diag_has_error else 0})"
		for exp in expected_diags:
			msg_sub = exp.get("message_contains")
			phase = exp.get("phase", expected_phase)
			match_found = False
			for d in checked_diags:
				if phase is not None and getattr(d, "phase", None) != phase:
					continue
				if msg_sub is not None and msg_sub not in d.message:
					continue
				match_found = True
				break
			if not match_found:
				return "FAIL (missing expected diagnostic)"
		return "ok"
	if checked_diags:
		return "FAIL (unexpected checker diagnostics)"

	if checked.diagnostics:
		expected_exit = expected.get("exit_code", 1)
		expected_phase = expected.get("phase", "typecheck")
		if expected.get("diagnostics") is None:
			return "FAIL (unexpected diagnostics during compilation)"
		if expected_exit != 1:
			return f"FAIL (diagnostics present, expected exit {expected_exit})"
		for exp in expected.get("diagnostics", []):
			msg_sub = exp.get("message_contains")
			phase = exp.get("phase", expected_phase)
			match_found = False
			for d in checked.diagnostics:
				if phase is not None and getattr(d, "phase", None) != phase:
					continue
				if msg_sub is not None and msg_sub not in d.message:
					continue
				match_found = True
				break
			if not match_found:
				return "FAIL (missing expected diagnostic)"
		return "ok"

	build_dir = BUILD_ROOT / case_dir.name
	run_args = expected.get("args", [])
	if needs_argv and not run_args:
		return "FAIL (argv main requires args in expected.json)"
	exit_code, stdout, stderr = _run_ir_with_clang(ir, build_dir, argv=run_args)
	if exit_code != 0 and stderr == "clang not available":
		return "FAIL (clang not available)"

	if checked.diagnostics:
		actual_exit = 1
		actual_stdout = ""
		actual_stderr = "\n".join(d.message for d in checked.diagnostics)
		expected_exit = expected.get("exit_code", actual_exit)
		if expected_phase not in (None, "checker", "codegen"):
			return f"FAIL (expected phase {expected_phase}, but diagnostics occurred during checker/codegen phase)"
		if expected_exit != actual_exit:
			return f"FAIL (checker/codegen phase exit {actual_exit}, expected {expected_exit})"
		if expected.get("stdout", actual_stdout) != actual_stdout:
			return "FAIL (checker/codegen phase stdout mismatch)"
		if expected.get("stderr", actual_stderr) != actual_stderr:
			return "FAIL (checker/codegen phase stderr mismatch)"
		return "ok"

	if exit_code != expected.get("exit_code", 0):
		return f"FAIL (exit {exit_code}, expected {expected.get('exit_code', 0)})"
	if stdout != expected.get("stdout", ""):
		return "FAIL (stdout mismatch)"
	if stderr != expected.get("stderr", ""):
		return "FAIL (stderr mismatch)"
	return "ok"


def _run_case_worker(case_dir: str) -> tuple[str, str]:
	path = Path(case_dir)
	try:
		status = _run_case(path)
	except Exception as err:  # pragma: no cover - worker guardrail
		return path.name, f"FAIL (worker exception: {err})"
	return path.name, status


def _run_case_chunk(case_dirs: list[str]) -> list[tuple[str, str]]:
	results: list[tuple[str, str]] = []
	for case_dir in case_dirs:
		results.append(_run_case_worker(case_dir))
	return results


def main(argv: Iterable[str] | None = None) -> int:
	ap = argparse.ArgumentParser(description="Run Drift-source codegen e2e cases (clang-based)")
	ap.add_argument(
		"cases",
		nargs="*",
		help="Specific test case names to run (directories under lang2/codegen/tests/e2e)",
	)
	ap.add_argument(
		"-j",
		"--jobs",
		type=str,
		default="auto",
		help="Run cases in parallel across N worker processes (N or 'auto', default: auto)",
	)
	ap.add_argument(
		"--work-size",
		type=int,
		default=4,
		help="Run this many cases sequentially per worker task (default: 4)",
	)
	ap.add_argument(
		"--ordered",
		action="store_true",
		help="Print results in deterministic case order instead of as they finish",
	)
	ap.add_argument(
		"--summarize",
		action="store_true",
		help="Print a summary line with test counts and elapsed time",
	)
	args = ap.parse_args(argv)
	start_time = time.monotonic() if args.summarize else None

	case_root = ROOT / "lang2" / "codegen" / "tests" / "e2e"
	case_dirs = (
		sorted(
			d
			for d in case_root.iterdir()
			if d.is_dir() and not d.name.startswith(".") and not d.name.startswith("__")
		)
		if case_root.exists()
		else []
	)
	if args.cases:
		names = set(args.cases)
		case_dirs = [d for d in case_dirs if d.name in names]

	failures: list[tuple[Path, str]] = []
	skipped: set[str] = set()
	if args.jobs == "auto":
		cpu_count = os.cpu_count() or 1
		jobs = max(1, cpu_count - 1)
	else:
		try:
			jobs = int(args.jobs)
		except ValueError:
			print(f"invalid --jobs value: {args.jobs!r} (expected integer or 'auto')", file=sys.stderr)
			return 2
		if jobs < 1:
			print(f"invalid --jobs value: {jobs} (must be >= 1 or 'auto')", file=sys.stderr)
			return 2
	if jobs == 1 or len(case_dirs) <= 1:
		for case_dir in case_dirs:
			status = _run_case(case_dir)
			print(f"{case_dir.name}: {status}")
			if status.startswith("FAIL"):
				failures.append((case_dir, status))
			elif status.startswith("skipped"):
				skipped.add(case_dir.name)
	else:
		if args.work_size < 1:
			print(f"invalid --work-size value: {args.work_size} (must be >= 1)", file=sys.stderr)
			return 2
		chunk_size = args.work_size
		chunks: list[list[str]] = []
		for i in range(0, len(case_dirs), chunk_size):
			chunks.append([str(p) for p in case_dirs[i : i + chunk_size]])
		with ProcessPoolExecutor(max_workers=jobs) as executor:
			futures = [executor.submit(_run_case_chunk, chunk) for chunk in chunks]
			if args.ordered:
				results: dict[str, str] = {}
				next_idx = 0
				for fut in as_completed(futures):
					for name, status in fut.result():
						results[name] = status
					# Emit any newly available results in deterministic order.
					while next_idx < len(case_dirs):
						case_dir = case_dirs[next_idx]
						status = results.get(case_dir.name)
						if status is None:
							break
						print(f"{case_dir.name}: {status}")
						if status.startswith("FAIL"):
							failures.append((case_dir, status))
						elif status.startswith("skipped"):
							skipped.add(case_dir.name)
						next_idx += 1
				for case_dir in case_dirs[next_idx:]:
					status = results.get(case_dir.name, "FAIL (missing result)")
					print(f"{case_dir.name}: {status}")
					if status.startswith("FAIL"):
						failures.append((case_dir, status))
					elif status.startswith("skipped"):
						skipped.add(case_dir.name)
			else:
				for fut in as_completed(futures):
					for name, status in fut.result():
						print(f"{name}: {status}")
						if status.startswith("FAIL"):
							failures.append((case_root / name, status))
						elif status.startswith("skipped"):
							skipped.add(name)

	if failures:
		for case, status in failures:
			print(f"[codegen e2e] {case.name}: {status}", file=sys.stderr)
		exit_code = 1
	else:
		exit_code = 0
	if start_time is not None:
		elapsed = time.monotonic() - start_time
		total = len(case_dirs)
		failed = len(failures)
		skipped_count = len(skipped)
		passed = total - failed - skipped_count
		print("============[ SUMMARY ]=============", file=sys.stderr)
		print(f"Tests: {total} ({passed} successful, {skipped_count} skipped, {failed} failed)", file=sys.stderr)
		print(f"Elapsed: {elapsed:.2f} seconds", file=sys.stderr)
		if skipped:
			print("Skipped tests:", file=sys.stderr)
			for case_dir in case_dirs:
				if case_dir.name in skipped:
					print(case_dir.name, file=sys.stderr)
	return exit_code


if __name__ == "__main__":
	raise SystemExit(main())
